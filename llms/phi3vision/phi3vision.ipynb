{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: phi35-vision-instruct\n",
      "Model type: chat-completion\n",
      "Model provider name: Phi\n",
      " Based on the image, the most appropriate category would be \"Violence\" as it depicts a group of people in a chaotic situation with a fire in the background, which suggests a violent event.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import sys\n",
    "# adding llms to the system path, make sure to configure the file azureopenai in llms folder with end point details\n",
    "sys.path.append('C:\\\\proddev\\\\research\\\\aiagentssamples\\\\llms\\\\')\n",
    "import azureopenai\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=os.environ[\"AZURE_OPENAI_PHI3VISION_ENDPOINT\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_OPENAI_PHI3VISION_ENDPOINT_KEY\"]),\n",
    ")\n",
    "\n",
    "model_info = client.get_model_info()\n",
    "print(\"Model name:\", model_info.model_name)\n",
    "print(\"Model type:\", model_info.model_type)\n",
    "print(\"Model provider name:\", model_info.model_provider_name)\n",
    "from urllib.request import urlopen, Request\n",
    "import os\n",
    "import base64\n",
    "\n",
    "#image_url = \"https://news.microsoft.com/source/wp-content/uploads/2024/04/The-Phi-3-small-language-models-with-big-potential-1-1900x1069.jpg\"\n",
    "image_url = \"https://news.microsoft.com/source/wp-content/uploads/2024/04/The-Phi-3-small-language-models-with-big-potential-1-1900x1069.jpg\"\n",
    "image_format = \"jpeg\"\n",
    "\n",
    "#request = Request(image_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "#image_data = base64.b64encode(urlopen(request).read()).decode(\"utf-8\")\n",
    "import base64\n",
    "#change this file path to your image file path\n",
    "with open('C:\\\\proddev\\\\hackathon2024\\\\Video_Content_Safety-8f987ac6-c095-43fd-a215-55c90780e48e-172613\\\\testdata\\\\output_frames_DemoMeetingRecording\\\\phi3\\\\voilance02.jfif', 'rb') as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "image_data = encoded_string\n",
    "data_url = f\"data:image/{image_format};base64,{image_data}\"\n",
    "import requests\n",
    "import IPython.display as Disp\n",
    "\n",
    "#Disp.Image(requests.get(image_url).content)\n",
    "\n",
    "from azure.ai.inference.models import TextContentItem, ImageContentItem, ImageUrl,SystemMessage, UserMessage\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant that can generate responses based on images\"),\n",
    "        UserMessage(content=[\n",
    "            TextContentItem(text=\"What is shown in this image? predict the most closet categories with confidence level between 0 to 10 from 1.Spam & Malicious \\\n",
    "2. Cultural Senstivity  \\\n",
    "3. Offensive Gestures \\\n",
    "4. Fake News \\\n",
    "5. Personal Information \\\n",
    "6. Confidentail Information  \\\n",
    "7. Violence  \\\n",
    "8. Hate Speech and Symbols\"),\n",
    "            ImageContentItem(image_url=ImageUrl(url=data_url))\n",
    "        ]),\n",
    "    ],\n",
    "    \n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
